/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

package org.apache.spark.sql.hudi.blob

import org.apache.hudi.storage.StorageConfiguration

import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeSet, Expression, UnaryExpression, Unevaluable}
import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, UnaryNode}
import org.apache.spark.sql.types.DataType

/**
 * Marker expression that references the blob column to prevent it from being pruned.
 * This expression is unevaluable and only exists for analysis purposes.
 */
case class BlobInputMarker(child: Expression) extends UnaryExpression with Unevaluable {
  override def dataType: DataType = child.dataType
  override protected def withNewChildInternal(newChild: Expression): Expression = copy(child = newChild)
}


/**
 * Logical plan node for batched blob reading.
 *
 * This node represents a plan to read blob data efficiently using batching.
 * It wraps a child plan and adds a temporary data column containing resolved blob bytes.
 *
 * The actual batched I/O is performed during physical execution by [[BatchedBlobReadExec]].
 *
 * @param child The child logical plan
 * @param blobColumnName Optional name of the column containing blob references
 * @param maxGapBytes Maximum gap between reads to batch together
 * @param lookaheadSize Number of rows to buffer for batch detection
 * @param storageConf Storage configuration for reading files
 * @param dataAttribute The generated data attribute - passed explicitly for ExprId stability
 * @param blobInputExpr Expression referencing the blob column to prevent pruning
 */
case class BatchedBlobRead(
    child: LogicalPlan,
    blobColumnName: Option[String],
    maxGapBytes: Int,
    lookaheadSize: Int,
    storageConf: StorageConfiguration[_],
    dataAttribute: Attribute,
    blobInputExpr: Seq[Expression]  // Expression that references blob column
) extends UnaryNode {

  /**
   * Output attributes include all child attributes plus the temporary data column.
   */
  override def output: Seq[Attribute] = child.output :+ dataAttribute

  /**
   * Produced attributes tells Spark that dataAttribute is generated by this node.
   */
  override def producedAttributes: AttributeSet = AttributeSet(Seq(dataAttribute))

  override protected def withNewChildInternal(newChild: LogicalPlan): LogicalPlan =
    copy(child = newChild)
}
